{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP勉強ノート"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "\n",
    "[第624回 Kaggle 自然言語データ分析ハッカソン by Team AI 11/23(土) - connpass](https://teamai.connpass.com/event/155039/)  \n",
    "\n",
    "[(保存版) 自然言語処理よちよち歩きの人がまず読むべき記事リスト by Team AI](https://qiita.com/daisuke-team-ai/items/d2e18f07a08d9b4cb783)  \n",
    "\n",
    "[Approaching (Almost) Any NLP Problem on Kaggle](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diver復習せねば！\n",
    "\n",
    "[Sprint 自然言語処理入門](https://diver.diveintocode.jp/curriculums/2010)  \n",
    "[Sprint 深層学習スクラッチリカレントニューラルネットワーク](https://diver.diveintocode.jp/curriculums/1982)  \n",
    "[Sprint ゲート付きリカレントニューラルネットワーク](https://diver.diveintocode.jp/curriculums/2006)  \n",
    "[Sprint課題 Seq2Seq](https://diver.diveintocode.jp/curriculums/2011)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB映画レビューデータセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T05:49:10.302375Z",
     "start_time": "2019-11-23T05:45:28.980500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-23 14:45:29--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "ai.stanford.edu (ai.stanford.edu) をDNSに問いあわせています... 171.64.68.10\n",
      "ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 84125825 (80M) [application/x-gzip]\n",
      "`aclImdb_v1.tar.gz' に保存中\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M   565KB/s 時間 3m 21s     \n",
      "\n",
      "2019-11-23 14:48:50 (409 KB/s) - `aclImdb_v1.tar.gz' へ保存完了 [84125825/84125825]\n",
      "\n",
      "Large Movie Review Dataset v1.0\n",
      "\n",
      "Overview\n",
      "\n",
      "This dataset contains movie reviews along with their associated binary\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\n",
      "sentiment classification. This document outlines how the dataset was\n",
      "gathered, and how to use the files provided. \n",
      "\n",
      "Dataset \n",
      "\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
      "documents for unsupervised learning. \n",
      "\n",
      "In the entire collection, no more than 30 reviews are allowed for any\n",
      "given movie because reviews for the same movie tend to have correlated\n",
      "ratings. Further, the train and test sets contain a disjoint set of\n",
      "movies, so no significant performance is obtained by memorizing\n",
      "movie-unique terms and their associated with observed labels.  In the\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
      "more neutral ratings are not included in the train/test sets. In the\n",
      "unsupervised set, reviews of any rating are included and there are an\n",
      "even number of reviews > 5 and <= 5.\n",
      "\n",
      "Files\n",
      "\n",
      "There are two top-level directories [train/, test/] corresponding to\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\n",
      "the reviews with binary labels positive and negative. Within these\n",
      "directories, reviews are stored in text files named following the\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
      "the star rating for that review on a 1-10 scale. For example, the file\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
      "example with unique id 200 and star rating 8/10 from IMDb. The\n",
      "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
      "omitted for this portion of the dataset.\n",
      "\n",
      "We also include the IMDb URLs for each review in a separate\n",
      "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
      "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
      "are unable to link directly to the review, but only to the movie's\n",
      "review page.\n",
      "\n",
      "In addition to the review text files, we include already-tokenized bag\n",
      "of words (BoW) features that were used in our experiments. These \n",
      "are stored in .feat files in the train/test directories. Each .feat\n",
      "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
      "data.  The feature indices in these files start from 0, and the text\n",
      "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
      "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
      "(the) appears 7 times in that review.\n",
      "\n",
      "LIBSVM page for details on .feat file format:\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
      "\n",
      "We also include [imdbEr.txt] which contains the expected rating for\n",
      "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
      "rating is a good way to get a sense for the average polarity of a word\n",
      "in the dataset.\n",
      "\n",
      "Citing the dataset\n",
      "\n",
      "When using this dataset please cite our ACL 2011 paper which\n",
      "introduces it. This paper also contains classification results which\n",
      "you may want to compare against.\n",
      "\n",
      "\n",
      "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "  month     = {June},\n",
      "  year      = {2011},\n",
      "  address   = {Portland, Oregon, USA},\n",
      "  publisher = {Association for Computational Linguistics},\n",
      "  pages     = {142--150},\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "}\n",
      "\n",
      "References\n",
      "\n",
      "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
      "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
      "636-659.\n",
      "\n",
      "Contact\n",
      "\n",
      "For questions/comments/corrections please contact Andrew Maas\n",
      "amaas@cs.stanford.edu\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# IMDBをカレントフォルダにダウンロード\n",
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# 解凍\n",
    "!tar zxf aclImdb_v1.tar.gz\n",
    "# aclImdb/train/unsupはラベル無しのため削除\n",
    "!rm -rf aclImdb/train/unsup\n",
    "'''\n",
    "# IMDBデータセットの説明を表示\n",
    "!cat aclImdb/README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T05:49:29.402282Z",
     "start_time": "2019-11-23T05:49:10.305597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "\n",
    "# ラベルの0,1と意味の対応の表示\n",
    "print(train_review.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T05:49:34.471744Z",
     "start_time": "2019-11-23T05:49:34.468297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\n"
     ]
    }
   ],
   "source": [
    "print(\"x : {}\".format(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF使い方メモ\n",
    "'''\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features = 5000, ngram_range=(2,2))\n",
    "\n",
    "x_train_tfidf_2gram = vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf_2gram = vectorizer.fit_transform(x_test)\n",
    "\n",
    "print(x_train_tfidf_2gram.shape)\n",
    "print(x_test_tfidf_2gram.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## southparklines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T07:57:36.463251Z",
     "start_time": "2019-11-23T07:57:32.989397Z"
    }
   },
   "outputs": [],
   "source": [
    "# パッケージのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import glob\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import date2num\n",
    "from matplotlib.dates import DateFormatter\n",
    "import os\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import sklearn.preprocessing as sp\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:02:15.778666Z",
     "start_time": "2019-11-23T08:02:15.670450Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"southparklines/All-seasons.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:02:16.466106Z",
     "start_time": "2019-11-23T08:02:16.458565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>You guys, you guys! Chef is going away. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>Going away? For how long?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Forever.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Chef</td>\n",
       "      <td>I'm sorry boys.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Chef said he's been bored, so he joining a gro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Season Episode Character                                               Line\n",
       "0     10       1      Stan         You guys, you guys! Chef is going away. \\n\n",
       "1     10       1      Kyle                        Going away? For how long?\\n\n",
       "2     10       1      Stan                                         Forever.\\n\n",
       "3     10       1      Chef                                  I'm sorry boys.\\n\n",
       "4     10       1      Stan  Chef said he's been bored, so he joining a gro..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:02:17.849927Z",
     "start_time": "2019-11-23T08:02:17.759008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>70896</td>\n",
       "      <td>70896</td>\n",
       "      <td>70896</td>\n",
       "      <td>70896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>3950</td>\n",
       "      <td>64301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>What?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6416</td>\n",
       "      <td>5271</td>\n",
       "      <td>9774</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Season Episode Character     Line\n",
       "count   70896   70896     70896    70896\n",
       "unique     19      19      3950    64301\n",
       "top         2      10   Cartman  What?\\n\n",
       "freq     6416    5271      9774      361"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cartman\n",
    "14%  \n",
    "Stan\n",
    "11%  \n",
    "Kyle\n",
    "10%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不均衡データのサンプリングについて\n",
    "アンダーとオーバー  \n",
    "https://takuti.me/note/adjusting-for-oversampling-and-undersampling/ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三人に絞る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:02:21.637310Z",
     "start_time": "2019-11-23T08:02:21.628475Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mainchara = df[df['Character'].isin(['Cartman', 'Stan', 'Kyle'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:02:22.379002Z",
     "start_time": "2019-11-23T08:02:22.369156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>You guys, you guys! Chef is going away. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>Going away? For how long?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Forever.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Chef said he's been bored, so he joining a gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>I'm gonna miss him.  I'm gonna miss Chef and I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Season Episode Character                                               Line\n",
       "0     10       1      Stan         You guys, you guys! Chef is going away. \\n\n",
       "1     10       1      Kyle                        Going away? For how long?\\n\n",
       "2     10       1      Stan                                         Forever.\\n\n",
       "4     10       1      Stan  Chef said he's been bored, so he joining a gro...\n",
       "9     10       1   Cartman  I'm gonna miss him.  I'm gonna miss Chef and I..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mainchara.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:02:39.937603Z",
     "start_time": "2019-11-23T08:02:39.930660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop word : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ryuji_sato/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# はじめて使う場合はストップワードをダウンロード\n",
    "import nltk\n",
    "stop_words = nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(\"stop word : {}\".format(stop_words)) # 'i', 'me', 'my', ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:03:12.578010Z",
     "start_time": "2019-11-23T08:03:12.470707Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryuji_sato/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>You guys, you guys! Chef is going away. \\n</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>Going away? For how long?\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Forever.\\n</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Chef said he's been bored, so he joining a gro...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>I'm gonna miss him.  I'm gonna miss Chef and I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Season Episode Character                                               Line  label\n",
       "0     10       1      Stan         You guys, you guys! Chef is going away. \\n      2\n",
       "1     10       1      Kyle                        Going away? For how long?\\n      1\n",
       "2     10       1      Stan                                         Forever.\\n      2\n",
       "4     10       1      Stan  Chef said he's been bored, so he joining a gro...      2\n",
       "9     10       1   Cartman  I'm gonna miss him.  I'm gonna miss Chef and I...      0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#LabelEncoderのインスタンスを生成\n",
    "le = LabelEncoder()\n",
    "#ラベルを覚えさせる\n",
    "le = le.fit(df_mainchara['Character'])\n",
    "#ラベルを整数に変換\n",
    "df_mainchara['label'] = le.transform(df_mainchara['Character'])\n",
    "df_mainchara.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:03:21.603294Z",
     "start_time": "2019-11-23T08:03:21.598748Z"
    }
   },
   "outputs": [],
   "source": [
    "df_line = df_mainchara[['label', 'Line']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:03:22.225351Z",
     "start_time": "2019-11-23T08:03:22.218635Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>You guys, you guys! Chef is going away. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Going away? For how long?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Forever.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Chef said he's been bored, so he joining a gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm gonna miss him.  I'm gonna miss Chef and I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               Line\n",
       "0      2         You guys, you guys! Chef is going away. \\n\n",
       "1      1                        Going away? For how long?\\n\n",
       "2      2                                         Forever.\\n\n",
       "4      2  Chef said he's been bored, so he joining a gro...\n",
       "9      0  I'm gonna miss him.  I'm gonna miss Chef and I..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_line.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:03:28.835621Z",
     "start_time": "2019-11-23T08:03:28.829201Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:03:30.262761Z",
     "start_time": "2019-11-23T08:03:30.259263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18414, 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:03:31.792698Z",
     "start_time": "2019-11-23T08:03:31.789005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6139, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:03:33.262225Z",
     "start_time": "2019-11-23T08:03:33.259168Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = df_train['Line']\n",
    "x_test = df_test['Line']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:42:02.463256Z",
     "start_time": "2019-11-23T08:42:02.460333Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = np.array(df_train['label'])\n",
    "y_test = np.array(df_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:42:04.956675Z",
     "start_time": "2019-11-23T08:42:04.953608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18414,)\n",
      "(6139,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:23:18.113851Z",
     "start_time": "2019-11-23T08:23:17.608120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18414, 5000)\n",
      "(6139, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features = 5000, ngram_range=(2,2))\n",
    "\n",
    "x_train_tfidf_2gram = vectorizer.fit_transform(x_train).toarray()\n",
    "x_test_tfidf_2gram = vectorizer.fit_transform(x_test).toarray()\n",
    "\n",
    "print(x_train_tfidf_2gram.shape)\n",
    "print(x_test_tfidf_2gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:31:25.784044Z",
     "start_time": "2019-11-23T08:31:25.567092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_66 (Dense)             (None, 1024)              5121024   \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 5,821,097\n",
      "Trainable params: 5,821,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_size = 5000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(1024, activation=tf.nn.relu, input_shape=(input_size,) ))\n",
    "model.add(keras.layers.Dense(512, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(256, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(64, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(32, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(8, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(4, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(2, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.softmax))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:33:26.497343Z",
     "start_time": "2019-11-23T08:33:26.457676Z"
    }
   },
   "outputs": [],
   "source": [
    "#コンパイル\n",
    "model.compile(optimizer=tf.optimizers.Adam(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimazerでエラーが出た\n",
    "  \n",
    "https://stackoverflow.com/questions/55318273/tensorflow-api-v2-train-has-no-attribute-adamoptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T08:40:10.038926Z",
     "start_time": "2019-11-23T08:40:08.079531Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18414 samples, validate on 6139 samples\n",
      "Epoch 1/40\n",
      " 2500/18414 [===>..........................] - ETA: 11s"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Received a label value of 2 which is outside the valid range of [0, 1).  Label values: 1 2 0 2 2 1 0 0 2 1 1 2 1 1 0 0 0 2 2 0 0 0 0 2 2 0 0 2 0 1 2 2 0 0 2 0 2 2 1 2 0 1 0 1 0 1 1 2 1 2 0 0 1 1 0 2 2 2 0 2 0 2 0 0 0 0 0 2 0 0 0 1 0 0 1 2 1 2 2 1 2 0 0 1 0 1 2 0 1 1 0 1 0 0 0 0 0 0 0 1 2 1 1 0 1 0 2 2 0 0 2 0 2 0 2 0 0 2 1 1 1 1 1 0 0 0 1 2 0 1 0 1 0 2 0 2 1 1 1 0 1 1 2 2 0 0 2 2 1 2 2 0 2 0 1 2 0 1 0 1 1 0 1 1 0 0 1 0 2 2 2 0 2 2 0 1 2 1 0 0 2 0 0 1 0 2 1 2 1 2 1 1 1 2 2 1 1 2 0 2 2 0 1 0 2 0 2 2 1 2 1 2 1 1 2 0 0 2 1 0 0 2 0 0 2 1 2 1 1 0 0 0 1 0 1 1 2 0 2 2 0 2 0 1 0 0 2 1 0 0 1 0 2 2 2 0 1 2 0 0 0 2 1 2 1 0 1 2 0 0 1 1 0 1 2 1 0 0 1 1 1 0 1 0 2 2 0 0 0 1 2 1 0 1 0 0 1 1 0 0 2 0 0 1 2 0 0 1 1 1 0 1 1 0 2 1 2 0 2 2 0 1 2 0 1 2 1 1 0 2 0 1 1 0 1 0 2 0 2 0 2 0 1 0 1 0 2 0 2 1 0 0 0 2 2 1 1 0 0 0 1 0 0 2 1 0 0 1 0 2 0 1 0 1 2 2 0 0 1 0 1 2 2 0 2 2 2 2 2 0 2 0 0 1 0 0 0 2 2 2 2 2 1 2 1 0 1 0 2 0 1 1 2 0 2 2 1 0 0 2 1 0 2 2 0 2 2 0 0 0 2 2 0 1 2 2 2 2 0 0 2 1 2 2 2 0 0 2 0 2 1 2 0 2 0 2 1 0 2 1 0 1 2 1 2 0 1 1 2 1 0 1 1 1 0 1 1 2 1 2 0 2 0 1 1 1 2 0 1 2 1 0 2 2 0 0 2 0 2 2 2 1 1 2 0 2 2 2 1 1 1 2 2 2 2 1 0 2 2 2 2 0 0 0 2 1 2 0 1 2 1 1 0 0 2 2 1 2 0 0 0 0 2 0 0 0 2 1 0 0 0 0 0 1 0 1 2 0 0 1 1 2 2 0 2 2 0 0 2 0 2 1 0 0 0 2 2 1 2 2 2 1 1 0 0 2 2 1 1 1 0 0 0 0 1 0 0 0 2 0 1 0 2 2 0 1 0 1 0 2 0 0 2 2 1 0 0 0 0 1 0 1 2 1 0 2 0 2 2 2 0 1 0 1 2 1 2 0 1 2 0 0 1 0 0 2 0 0 0 0 1 1 0 2 0 1 0 2 0 2 0 2 0 0 0 0 2 2 0 0 1 1 0 0 0 1 0 0 2 1 0 2 2 2 1 1 1 0 0 2 1 0 0 2 0 2 1 0 2 2 2 0 2 2 1 0 0 0 0 0 1 2 0 0 0 1 2 1 2 0 2 2 2 1 0 0 1 0 0 2 2 0 1 0 2 0 2 1 0 2 2 0 0 0 1 1 2 2 0 0 1 0 0 0 1 2 1 1 2 0 1 2 1 2 1 1 2 0 2 2 1 0 1 0 2 0 1 0 1 2 2 0 1 0 0 2 1 0 1 1 1 0 2 2 1 1 0 2 2 0 1 1 0 2 2 0 0 2 1 1 2 0 1 2 2 2 2 2 2 1 1 2 0 0 0 2 1 0 2 0 2 0 2 1 0 0 2 2 2 0 1 2 0 2 1 0 2 0 0 2 0 2 0 2 0 0 2 2 1 2 2 0 0 0 0 2 1 0 0 1 0 2 0 0 0 0 2 2 1 1 1 1 1 0 2 0 2 2 0 0 0 0 0 2 2 0 2 2 0 0 0 2 1 1 0 2 1 2 2 0 1 2 0 2 0 1 1 1 0 1 0 1 1 2 1 1 0 1 1 0 2 0 2 0 1 1 0 1 0 0 1 2 1 1 0 1 0 2 2 1 2 0 1 1 2 2 2 0 2 0 0 0 2 1 2 1 0 1 0 0 1 1 0 0 2 0 1 0 0 0 1 2 2 1 2 2 0 2 1 0 0 0 0 1 2 1 0 1 2 0 2 1 2 2 2 2 2 1 0 0 1 0 2 1 1 2 2 0 0 2 2 0 0 2 1 0 1 1 0 1 2 1 2 2 2 2 2 1 0 0 0 0 2 1 1 2 1 1 1 0 0 0 0 1 2 1 0 2 2 0 0 1 2 0 1 0 1 0 2 1 1 0 0 0 0 0 0 2 1 1 0 2 0 0 0 1 1 2 0 0 2 0 0 1 1 0 0 0 0 2 0 2 0 1 1 2 0 2 0 2 1 0 2 2 0 0 2 0 2 0 1 1 0 0 1 2 1 1 0 2 2 0 0 2 0 0 0 1 1 0 2 0 1 0 0 0 0 0 0 1 2 1 0 1 0 0 0 2 1 1 0 0 0 2 2 2 0 2 0 0 1 2 0 0 1 0 0 2 2 0 1 2 0 2 0 1 2 2 0 2 2 0 0 2 2 2 0 1 2 2 0 2 0 2 1 0 2 0 0 1 1 0 2 0 1 2 1 2 2 0 2 2 1 0 0 1 2 0 2 0 0 2 2 0 2 0 0 2 0 0 2 0 0 0 2 2 0 0 2 1 1 1 1 0 2 0 2 0 0 0 0 1 0 1 2 1 0 1 0 2 1 0 1 0 2 2 1 0 2 2 2 2 1 2 0 1 1 0 0 2 2 1 2 1 2 2 2 1 1 2 2 0 1 0 2 1 0 2 0 0 2 1 2 0 0 0 1 0 2 2 0 0 2 2 1 0 0 0 0 1 1 1 0 1 2 1 1 0 0 0 2 2 0 0 2 2 2 1 1 1 2 1 0 0 1 0 1 2 1 0 2 1 2 0 0 2 2 1 0 0 1 0 0 0 2 1 1 0 2 0 0 2 0 2 2 0 0 1 0 2 2 1 2 2 0 0 1 2 1 0 1 1 0 1 2 0 1 2 2 1 1 1 2 0 1 0 1 0 0 0 1 2 2 0 1 0 1 2 1 1 1 2 0 0 2 0 2 0 2 1 0 0 1 0 0 0 1 2 0 2 2 0 2 2 0 0 1 2 1 2 1 1 0 0 1 1 1 0 0 2 2 2 1 0 1 0 0 0 0 2 2 2 2 1 1 0 0 2 0 2 1 1 2 2 1 0 2 0 2 1 2 2 0 1 1 0 1 1 2 1 1 0 1 0 2 2 0 0 0 1 2 2 0 1 2 0 1 2 2 0 2 1 1 2 0 1 1 2 0 2 2 1 0 1 0 2 0 1 2 0 1 1 1 2 2 2 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 1 2 2 1 2 1 1 1 1 0 0 2 0 2 2 1 1 0 1 0 1 0 2 2 0 0 2 2 2 1 0 0 2 0 2 0 0 2 2 0 1 0 1 0 0 1 0 0 2 1 2 0 0 2 0 0 0 0 0 1 2 1 0 1 0 1 1 0 2 1 0 1 0 0 0 2 2 2 2 1 0 0 1 0 0 0 2 1 0 0 0 1 0 1 0 2 1 1 2 0 1 1 0 0 0 0 1 0 0 2 0 2 1 2 2 0 0 2 2 2 0 0 0 0 0 1 1 0 2 2 2 0 0 1 2 2 2 0 2 2 1 0 0 2 0 0 0 1 2 0 1 1 2 0 2 1 2 2 1 1 2 0 0 1 1 1 0 0 0 1 0 1 2 2 0 1 1 2 2 2 0 0 0 1 0 2 2 1 0 1 2 1 2 0 1 2 0 0 0 0 0 0 1 0 2 2 0 0 0 0 0 2 2 1 0 0 0 1 0 0 0 1 2 0 1 2 2 0 0 0 2 0 2 1 0 1 1 1 0 1 1 2 2 0 1 1 0 0 2 1 2 0 2 0 0 1 1 0 1 1 1 0 0 0 0 2 2 0 1 1 1 2 2 1 2 1 2 0 2 0 1 2 0 1 0 0 0 0 0 0 2 1 2 0 0 2 0 0 0 2 2 2 1 1 2 2 1 1 0 1 1 0 0 0 0 0 1 0 1 1 2 0 0 1 0 2 2 2 1 1 0 0 2 0 0 1 2 0 2 1 2 0 0 2 0 0 2 2 2 2 2 1 2 0 2 0 0 1 2 0 0 1 1 2 2 0 0 1 2 2 2 0 0 1 2 0 2 2 2 2 0 0 1 0 1 0 1 1 1 1 2 0 2 0 2 0 0 0 0 0 1 2 1 2 2 2 0 0 2 1 0 2 0 1 1 0 0 1 2 2 1 0 1 2 0 1 0 0 0 1 2 2 0 0 1 1 0 0 0 2 2 2 0 2 0 1 2 0 2 1 1 2 1 2 0 0 0 2 2 0 0 2 1 1 0 2 2 0 1 1 2 0 2 0 2 0 0 1 2 2 1 2 0 0 1 2 2 0 2 2 0 2 1 1 2 1 2 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 2 1 2 2 0 2 0 2 0 1 2 2 2 1 0 0 1 1 2 1 1 0 1 2 2 0 1 1 2 1 2 0 0 0 2 2 0 2 1 1 1 1 0 1 0 0 0 0 2 0 0 2 2 0 0 2 2 0 2 1 0 1 0 1 0 2 0 2 1 1 1 1 2 1 1 2 0 1 1 0 0 2 0 2 1 2 0 1 2 0 2 0 2 2 1 0 2 2 1 0 2 1 2 1 2 1 1 2 2 0 0 0 1 2 0 0 0 1 2 0 1 0 1 0 1 0 0 0 1 0 2 0 2 1 0 2 1 0 0 0 0 1 1 0 2 1 0 0 2 2 2 0 1 1 0 0 2 2 0 2 1 2 0 1 2 0 1 1 0 2 1 1 0 0 1 0 1 0 1 0 2 1 1 2 2 2 0 0 2 2 2 2 2 1 1 2 2 0 0 0 0 2 1 1 0 1 2 0 2 0 2 1 0 2 1 2 1 1 0 0 2 1 0 2 2 0 0 0 0 0 2 0 0 2 0 0 0 2 0 0 1 0 1 1 0 0 1 0 2 2 1 1 0 0 0 0 2 2 1 0 0 0 0 1 0 2 2 1 0 1 0 0 2 2 2 2 1 0 1 1 0 2 1 0 0 0 2 1 2 1 0 2 0 1 2 2 2 2 2 1 2 1 0 0 1 2 0 0 1 2 0 0 0 0 0 0 2 1 2 2 0 2 0 1 2 1 1 0 2 0 0 1 2 2 1 0 2 0 0 0 2 1 2 0 2 2 1 1 1 0 1 2 1 1 2 0 0 0 2 1 2 1 2 0 0 2 2 0 2 0 1 2 1 1 0 0 0 1 0 2 1 2 2 0 0 0 0 0 1 1 2 2 0 0 2 1 0 2 1 1 1 0 0 1 1 0 2 1 0 0 0 2 0 0\n\t [[node loss/dense_76_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at /Users/ryuji_sato/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_7443]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-79d23584fdbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_tfidf_2gram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Received a label value of 2 which is outside the valid range of [0, 1).  Label values: 1 2 0 2 2 1 0 0 2 1 1 2 1 1 0 0 0 2 2 0 0 0 0 2 2 0 0 2 0 1 2 2 0 0 2 0 2 2 1 2 0 1 0 1 0 1 1 2 1 2 0 0 1 1 0 2 2 2 0 2 0 2 0 0 0 0 0 2 0 0 0 1 0 0 1 2 1 2 2 1 2 0 0 1 0 1 2 0 1 1 0 1 0 0 0 0 0 0 0 1 2 1 1 0 1 0 2 2 0 0 2 0 2 0 2 0 0 2 1 1 1 1 1 0 0 0 1 2 0 1 0 1 0 2 0 2 1 1 1 0 1 1 2 2 0 0 2 2 1 2 2 0 2 0 1 2 0 1 0 1 1 0 1 1 0 0 1 0 2 2 2 0 2 2 0 1 2 1 0 0 2 0 0 1 0 2 1 2 1 2 1 1 1 2 2 1 1 2 0 2 2 0 1 0 2 0 2 2 1 2 1 2 1 1 2 0 0 2 1 0 0 2 0 0 2 1 2 1 1 0 0 0 1 0 1 1 2 0 2 2 0 2 0 1 0 0 2 1 0 0 1 0 2 2 2 0 1 2 0 0 0 2 1 2 1 0 1 2 0 0 1 1 0 1 2 1 0 0 1 1 1 0 1 0 2 2 0 0 0 1 2 1 0 1 0 0 1 1 0 0 2 0 0 1 2 0 0 1 1 1 0 1 1 0 2 1 2 0 2 2 0 1 2 0 1 2 1 1 0 2 0 1 1 0 1 0 2 0 2 0 2 0 1 0 1 0 2 0 2 1 0 0 0 2 2 1 1 0 0 0 1 0 0 2 1 0 0 1 0 2 0 1 0 1 2 2 0 0 1 0 1 2 2 0 2 2 2 2 2 0 2 0 0 1 0 0 0 2 2 2 2 2 1 2 1 0 1 0 2 0 1 1 2 0 2 2 1 0 0 2 1 0 2 2 0 2 2 0 0 0 2 2 0 1 2 2 2 2 0 0 2 1 2 2 2 0 0 2 0 2 1 2 0 2 0 2 1 0 2 1 0 1 2 1 2 0 1 1 2 1 0 1 1 1 0 1 1 2 1 2 0 2 0 1 1 1 2 0 1 2 1 0 2 2 0 0 2 0 2 2 2 1 1 2 0 2 2 2 1 1 1 2 2 2 2 1 0 2 2 2 2 0 0 0 2 1 2 0 1 2 1 1 0 0 2 2 1 2 0 0 0 0 2 0 0 0 2 1 0 0 0 0 0 1 0 1 2 0 0 1 1 2 2 0 2 2 0 0 2 0 2 1 0 0 0 2 2 1 2 2 2 1 1 0 0 2 2 1 1 1 0 0 0 0 1 0 0 0 2 0 1 0 2 2 0 1 0 1 0 2 0 0 2 2 1 0 0 0 0 1 0 1 2 1 0 2 0 2 2 2 0 1 0 1 2 1 2 0 1 2 0 0 1 0 0 2 0 0 0 0 1 1 0 2 0 1 0 2 0 2 0 2 0 0 0 0 2 2 0 0 1 1 0 0 0 1 0 0 2 1 0 2 2 2 1 1 1 0 0 2 1 0 0 2 0 2 1 0 2 2 2 0 2 2 1 0 0 0 0 0 1 2 0 0 0 1 2 1 2 0 2 2 2 1 0 0 1 0 0 2 2 0 1 0 2 0 2 1 0 2 2 0 0 0 1 1 2 2 0 0 1 0 0 0 1 2 1 1 2 0 1 2 1 2 1 1 2 0 2 2 1 0 1 0 2 0 1 0 1 2 2 0 1 0 0 2 1 0 1 1 1 0 2 2 1 1 0 2 2 0 1 1 0 2 2 0 0 2 1 1 2 0 1 2 2 2 2 2 2 1 1 2 0 0 0 2 1 0 2 0 2 0 2 1 0 0 2 2 2 0 1 2 0 2 1 0 2 0 0 2 0 2 0 2 0 0 2 2 1 2 2 0 0 0 0 2 1 0 0 1 0 2 0 0 0 0 2 2 1 1 1 1 1 0 2 0 2 2 0 0 0 0 0 2 2 0 2 2 0 0 0 2 1 1 0 2 1 2 2 0 1 2 0 2 0 1 1 1 0 1 0 1 1 2 1 1 0 1 1 0 2 0 2 0 1 1 0 1 0 0 1 2 1 1 0 1 0 2 2 1 2 0 1 1 2 2 2 0 2 0 0 0 2 1 2 1 0 1 0 0 1 1 0 0 2 0 1 0 0 0 1 2 2 1 2 2 0 2 1 0 0 0 0 1 2 1 0 1 2 0 2 1 2 2 2 2 2 1 0 0 1 0 2 1 1 2 2 0 0 2 2 0 0 2 1 0 1 1 0 1 2 1 2 2 2 2 2 1 0 0 0 0 2 1 1 2 1 1 1 0 0 0 0 1 2 1 0 2 2 0 0 1 2 0 1 0 1 0 2 1 1 0 0 0 0 0 0 2 1 1 0 2 0 0 0 1 1 2 0 0 2 0 0 1 1 0 0 0 0 2 0 2 0 1 1 2 0 2 0 2 1 0 2 2 0 0 2 0 2 0 1 1 0 0 1 2 1 1 0 2 2 0 0 2 0 0 0 1 1 0 2 0 1 0 0 0 0 0 0 1 2 1 0 1 0 0 0 2 1 1 0 0 0 2 2 2 0 2 0 0 1 2 0 0 1 0 0 2 2 0 1 2 0 2 0 1 2 2 0 2 2 0 0 2 2 2 0 1 2 2 0 2 0 2 1 0 2 0 0 1 1 0 2 0 1 2 1 2 2 0 2 2 1 0 0 1 2 0 2 0 0 2 2 0 2 0 0 2 0 0 2 0 0 0 2 2 0 0 2 1 1 1 1 0 2 0 2 0 0 0 0 1 0 1 2 1 0 1 0 2 1 0 1 0 2 2 1 0 2 2 2 2 1 2 0 1 1 0 0 2 2 1 2 1 2 2 2 1 1 2 2 0 1 0 2 1 0 2 0 0 2 1 2 0 0 0 1 0 2 2 0 0 2 2 1 0 0 0 0 1 1 1 0 1 2 1 1 0 0 0 2 2 0 0 2 2 2 1 1 1 2 1 0 0 1 0 1 2 1 0 2 1 2 0 0 2 2 1 0 0 1 0 0 0 2 1 1 0 2 0 0 2 0 2 2 0 0 1 0 2 2 1 2 2 0 0 1 2 1 0 1 1 0 1 2 0 1 2 2 1 1 1 2 0 1 0 1 0 0 0 1 2 2 0 1 0 1 2 1 1 1 2 0 0 2 0 2 0 2 1 0 0 1 0 0 0 1 2 0 2 2 0 2 2 0 0 1 2 1 2 1 1 0 0 1 1 1 0 0 2 2 2 1 0 1 0 0 0 0 2 2 2 2 1 1 0 0 2 0 2 1 1 2 2 1 0 2 0 2 1 2 2 0 1 1 0 1 1 2 1 1 0 1 0 2 2 0 0 0 1 2 2 0 1 2 0 1 2 2 0 2 1 1 2 0 1 1 2 0 2 2 1 0 1 0 2 0 1 2 0 1 1 1 2 2 2 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 1 2 2 1 2 1 1 1 1 0 0 2 0 2 2 1 1 0 1 0 1 0 2 2 0 0 2 2 2 1 0 0 2 0 2 0 0 2 2 0 1 0 1 0 0 1 0 0 2 1 2 0 0 2 0 0 0 0 0 1 2 1 0 1 0 1 1 0 2 1 0 1 0 0 0 2 2 2 2 1 0 0 1 0 0 0 2 1 0 0 0 1 0 1 0 2 1 1 2 0 1 1 0 0 0 0 1 0 0 2 0 2 1 2 2 0 0 2 2 2 0 0 0 0 0 1 1 0 2 2 2 0 0 1 2 2 2 0 2 2 1 0 0 2 0 0 0 1 2 0 1 1 2 0 2 1 2 2 1 1 2 0 0 1 1 1 0 0 0 1 0 1 2 2 0 1 1 2 2 2 0 0 0 1 0 2 2 1 0 1 2 1 2 0 1 2 0 0 0 0 0 0 1 0 2 2 0 0 0 0 0 2 2 1 0 0 0 1 0 0 0 1 2 0 1 2 2 0 0 0 2 0 2 1 0 1 1 1 0 1 1 2 2 0 1 1 0 0 2 1 2 0 2 0 0 1 1 0 1 1 1 0 0 0 0 2 2 0 1 1 1 2 2 1 2 1 2 0 2 0 1 2 0 1 0 0 0 0 0 0 2 1 2 0 0 2 0 0 0 2 2 2 1 1 2 2 1 1 0 1 1 0 0 0 0 0 1 0 1 1 2 0 0 1 0 2 2 2 1 1 0 0 2 0 0 1 2 0 2 1 2 0 0 2 0 0 2 2 2 2 2 1 2 0 2 0 0 1 2 0 0 1 1 2 2 0 0 1 2 2 2 0 0 1 2 0 2 2 2 2 0 0 1 0 1 0 1 1 1 1 2 0 2 0 2 0 0 0 0 0 1 2 1 2 2 2 0 0 2 1 0 2 0 1 1 0 0 1 2 2 1 0 1 2 0 1 0 0 0 1 2 2 0 0 1 1 0 0 0 2 2 2 0 2 0 1 2 0 2 1 1 2 1 2 0 0 0 2 2 0 0 2 1 1 0 2 2 0 1 1 2 0 2 0 2 0 0 1 2 2 1 2 0 0 1 2 2 0 2 2 0 2 1 1 2 1 2 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 2 1 2 2 0 2 0 2 0 1 2 2 2 1 0 0 1 1 2 1 1 0 1 2 2 0 1 1 2 1 2 0 0 0 2 2 0 2 1 1 1 1 0 1 0 0 0 0 2 0 0 2 2 0 0 2 2 0 2 1 0 1 0 1 0 2 0 2 1 1 1 1 2 1 1 2 0 1 1 0 0 2 0 2 1 2 0 1 2 0 2 0 2 2 1 0 2 2 1 0 2 1 2 1 2 1 1 2 2 0 0 0 1 2 0 0 0 1 2 0 1 0 1 0 1 0 0 0 1 0 2 0 2 1 0 2 1 0 0 0 0 1 1 0 2 1 0 0 2 2 2 0 1 1 0 0 2 2 0 2 1 2 0 1 2 0 1 1 0 2 1 1 0 0 1 0 1 0 1 0 2 1 1 2 2 2 0 0 2 2 2 2 2 1 1 2 2 0 0 0 0 2 1 1 0 1 2 0 2 0 2 1 0 2 1 2 1 1 0 0 2 1 0 2 2 0 0 0 0 0 2 0 0 2 0 0 0 2 0 0 1 0 1 1 0 0 1 0 2 2 1 1 0 0 0 0 2 2 1 0 0 0 0 1 0 2 2 1 0 1 0 0 2 2 2 2 1 0 1 1 0 2 1 0 0 0 2 1 2 1 0 2 0 1 2 2 2 2 2 1 2 1 0 0 1 2 0 0 1 2 0 0 0 0 0 0 2 1 2 2 0 2 0 1 2 1 1 0 2 0 0 1 2 2 1 0 2 0 0 0 2 1 2 0 2 2 1 1 1 0 1 2 1 1 2 0 0 0 2 1 2 1 2 0 0 2 2 0 2 0 1 2 1 1 0 0 0 1 0 2 1 2 2 0 0 0 0 0 1 1 2 2 0 0 2 1 0 2 1 1 1 0 0 1 1 0 2 1 0 0 0 2 0 0\n\t [[node loss/dense_76_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at /Users/ryuji_sato/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_7443]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_tfidf_2gram,\n",
    "                    y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=2500,\n",
    "                    validation_data=(x_test_tfidf_2gram, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "エピソードごとに傾向？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
